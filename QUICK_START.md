# Quick Start: Setting Up Groq (Primary LLM)

## ðŸš¨ Current Issue

Your system is falling back to Gemini because **GROQ_API_KEY is not set**. Logs show:
```
[LLM Provider] usa_f1 / question_selection â†’ gemini (gemini-1.5-flash)
```

It should say:
```
[LLM Provider] usa_f1 / question_selection â†’ groq (llama-3.1-8b-instant)
```

## âœ… Solution: Add Groq API Key

### Step 1: Get Your FREE Groq API Key

1. Visit: https://console.groq.com/
2. Sign up (it's FREE, no credit card required)
3. Go to "API Keys" section
4. Click "Create API Key"
5. Copy your key (starts with `gsk_...`)

### Step 2: Create .env.local File

Create a file named `.env.local` in your project root with:

```bash
# ===========================
# PRIMARY LLM: Groq (FREE)
# ===========================
GROQ_API_KEY=gsk_your_actual_groq_key_here

# ===========================
# FALLBACK: Gemini
# ===========================
GEMINI_API_KEY=your_gemini_key_here
GEMINI_MODEL=gemini-2.0-flash-exp

# ===========================
# Your existing Firebase keys
# ===========================
NEXT_PUBLIC_FIREBASE_API_KEY=...
# (copy from your existing config)
```

### Step 3: Restart Your Dev Server

```bash
# Stop the current server (Ctrl+C)
npm run dev
```

### Step 4: Verify Groq is Active

After restart, you should see:
```
[LLM Provider] usa_f1 / question_selection â†’ groq (llama-3.1-8b-instant)
[LLM Provider] usa_f1 / answer_scoring â†’ groq (llama-3.3-70b-versatile)
```

## ðŸ”¥ Why Groq?

**FREE Tier Benefits:**
- âœ… 533 interviews per day (vs 115 with Gemini)
- âœ… <1 second response time (vs 2-3s with Gemini)
- âœ… 30 requests per minute
- âœ… No token limits
- âœ… $0 cost on free tier

**Models Used:**
- **Question Selection**: Llama 3.1 8B Instant (fast, lightweight)
- **Answer Scoring**: Llama 3.3 70B Versatile (powerful, accurate)

## ðŸ› ï¸ Troubleshooting

### If You Don't Have .env.local

Copy the template:
```bash
cp .env.local.template .env.local
```

Then edit `.env.local` and add your actual API keys.

### If Groq Still Not Selected

Check that:
1. âœ… `.env.local` exists in project root (not in `src/`)
2. âœ… Key starts with `gsk_`
3. âœ… No spaces around the `=` sign
4. âœ… Dev server was restarted after adding the key

### If Gemini Model Error

The old model name (`gemini-1.5-flash`) is deprecated. I've updated it to:
- `gemini-2.0-flash-exp` (current working model)

This is now the fallback if Groq is unavailable.

## ðŸ“Š Expected Behavior

**With Groq (Correct):**
```
[LLM Provider] usa_f1 / question_selection â†’ groq (llama-3.1-8b-instant)
[Question Service] bank question selected: Intelligent LLM selection based on context
[LLM Provider] usa_f1 / answer_scoring â†’ groq (llama-3.3-70b-versatile)
```

**Without Groq (Current - Fallback):**
```
[LLM Provider] usa_f1 / question_selection â†’ gemini (gemini-2.0-flash-exp)
[Question Service] bank question selected: Rule-based fallback
```

## ðŸŽ¯ Current Status

- âœ… Code is ready for Groq
- âœ… Gemini model name fixed (`gemini-2.0-flash-exp`)
- â³ **Need to add GROQ_API_KEY to .env.local**
- â³ **Then restart dev server**

## ðŸ“ Template Files

I've created:
1. **`.env.local.template`** - Copy this to `.env.local` and fill in your keys
2. **`ENV_SETUP_GUIDE.md`** - Complete environment variable documentation

## ðŸš€ Next Steps

1. Get Groq API key from https://console.groq.com/
2. Create `.env.local` with your keys
3. Restart dev server
4. Verify logs show `â†’ groq` instead of `â†’ gemini`

---

**Need help?** Check `ENV_SETUP_GUIDE.md` for detailed instructions!
